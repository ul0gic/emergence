// This file was generated by [ts-rs](https://github.com/Aleph-Alpha/ts-rs). Do not edit this file manually.
import type { AgentId } from "./AgentId";
import type { JsonValue } from "../serde_json/JsonValue";

/**
 * Record of a single agent decision, published by the runner after each tick.
 *
 * Captures the full context of a decision: which agent, what action was chosen,
 * how it was decided (LLM, rule engine, night cycle, timeout), and diagnostics
 * including the prompt, raw response, token usage, cost, and latency.
 *
 * Published to NATS by the runner and collected by the engine for the Observer
 * REST API.
 */
export type DecisionRecord = { 
/**
 * The agent who made this decision.
 */
agent_id: AgentId, 
/**
 * The tick this decision was for.
 */
tick: bigint, 
/**
 * How the decision was made: `"llm"`, `"rule_engine"`, `"night_cycle"`, `"timeout"`.
 */
decision_source: string, 
/**
 * The action type chosen.
 */
action_type: string, 
/**
 * The action parameters as JSON.
 */
action_params: JsonValue, 
/**
 * LLM backend used (if LLM decision).
 */
llm_backend: string | null, 
/**
 * Model ID used (if LLM decision).
 */
model: string | null, 
/**
 * Input/prompt tokens (if LLM decision).
 */
prompt_tokens: number | null, 
/**
 * Output/completion tokens (if LLM decision).
 */
completion_tokens: number | null, 
/**
 * Estimated cost in USD (if LLM decision).
 */
cost_usd: number | null, 
/**
 * LLM response latency in milliseconds (if LLM decision).
 */
latency_ms: bigint | null, 
/**
 * The raw LLM response text (if LLM decision). Truncated to 4000 chars.
 */
raw_llm_response: string | null, 
/**
 * The assembled prompt sent to the LLM (if LLM decision). Truncated to 8000 chars.
 */
prompt_sent: string | null, 
/**
 * Which rule matched (if rule\_engine decision).
 */
rule_matched: string | null, 
/**
 * Timestamp of the decision.
 */
created_at: string, };
